{% extends "base.html" %}

{% block content %}
<style>
    .graph {
        width: 100%;
    }
</style>
<h1 class="text-primary">Heart Failure Prediction</h1>
<h2>Problem Definition</h2>

<p>The dataset of interest in this report is the <a
        href="https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data">Heart Failure Prediction</a>
    dataset on Kaggle. The target variable is <b>DEATH_EVENT</b>, whether the patient deceased during the
    variable-length follow-up period.</p>

<p>A total of 5 significant features were selected, all of them numeric:</p>
<ul>
    <li>time</li>
    <li>age</li>
    <li>serum_creatinine</li>
    <li>serum_sodium</li>
    <li>ejection_fraction</li>
</ul>

<h2>Exploratory Data Analysis</h2>
<h3 class="text-secondary">Evaluating Significant Features</h3>
<p>There are a total of 12 features (aside from the target variable), so picking the top 5 features from each routine
    seems like a reasonable number. The following automated feature selection routines were used to identify significant
    features:</p>
<ul>
    <li>Forward feature selection (FFS)</li>
    <li>Random forest feature importance</li>
    <li>Chi-squared test</li>
</ul>
<p>It was found that recursive feature elimination (RFE) ranked all 12 features equally, so its results are
    discarded.</p>

<p>The features selected by the remaining routines are as follows:</p>

<table class="table table-striped table-bordered">
    <thead class="thead-dark">
    <th></th>
    <th>Top 5 Features</th>
    </thead>
    <tbody>
    <tr>
        <td>FFS</td>
        <td>time<br>serum_creatinine<br>ejection_fraction<br>age<br>serum_sodium</td>
    </tr>
    <tr>
        <td>Random Forest<br>Feature Importance</td>
        <td>time<br>serum_creatinine<br>creatinine_phosphokinase<br>age<br>serum_sodium</td>
    </tr>
    <tr>
        <td>Chi-squared Test</td>
        <td>platelets<br>time<br>creatinine_phosphokinase<br>ejection_fraction<br>age</td>
    </tr>
    </tbody>
</table>

<p>Models were developed with features selected by all of these routines, and it was found that creatinine_phosphokinase
    was also insignificant, leaving 5 significant variables to work with.</p>

<h3 class="text-secondary">Feature Information</h3>
<p>The data distribution of the target variable and the 5 significant features are as follows:</p>
<table class="table table-striped table-bordered">
    <thead class="thead-dark">
    <tr>
        <th></th>
        <th>count</th>
        <th>mean</th>
        <th>std</th>
        <th>min</th>
        <th>25%</th>
        <th>50%</th>
        <th>75%</th>
        <th>max</th>
    </tr>
    </thead>
    <tbody>
    <tr>
        <td>DEATH_EVENT</td>
        <td>299.0</td>
        <td>0.3211</td>
        <td>0.4677</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>1.0</td>
        <td>1.0</td>
    </tr>
    <tr>
        <td>time</td>
        <td>299.0</td>
        <td>130.2609</td>
        <td>77.6142</td>
        <td>4.0</td>
        <td>73.0</td>
        <td>115.0</td>
        <td>203.0</td>
        <td>285.0</td>
    </tr>
    <tr>
        <td>age</td>
        <td>299.0</td>
        <td>60.8339</td>
        <td>11.8948</td>
        <td>40.0</td>
        <td>51.0</td>
        <td>60.0</td>
        <td>70.0</td>
        <td>95.0</td>
    </tr>
    <tr>
        <td>serum_creatinine</td>
        <td>299.0</td>
        <td>1.3939</td>
        <td>1.0345</td>
        <td>0.5</td>
        <td>0.9</td>
        <td>1.1</td>
        <td>1.4</td>
        <td>9.4</td>
    </tr>
    <tr>
        <td>serum_sodium</td>
        <td>299.0</td>
        <td>136.6254</td>
        <td>4.4125</td>
        <td>113.0</td>
        <td>134.0</td>
        <td>137.0</td>
        <td>140.0</td>
        <td>148.0</td>
    </tr>
    <tr>
        <td>ejection_fraction</td>
        <td>299.0</td>
        <td>38.0836</td>
        <td>11.8348</td>
        <td>14.0</td>
        <td>30.0</td>
        <td>38.0</td>
        <td>45.0</td>
        <td>80.0</td>
    </tr>
    </tbody>
</table>
<p>The target variable DEATH_EVENT is a Boolean variable indicating whether the patient deceased during the follow-up
    period. Of the total number of rows in the dataset, roughly one-third of them has deceased while the remaining rows
    survived the entire follow-up period.</p>
<img class="graph" alt="Histogram of DEATH_EVENT" src="https://i.imgur.com/DczcJxT.png">
<p>The “time” feature measures the follow-up period in days. It has a strong negative correlation with DEATH_EVENT: the
    longer the follow-up period, the less likely the patient is to be deceased during the follow-up period. This is
    contrary to what one might expect, as it’s logical to assume that a patient is more likely to pass away during the
    follow-up period if the period is longer. Considering this information, one might hypothesize that patients who have
    a high probability of passing away given their parameters for the other features are likely to pass away early on,
    and the longer a patient survives the more stable their implied condition.</p>
<img class="graph" alt="Histogram of time versus DEATH_EVENT" src="https://i.imgur.com/rnBTaiY.png">
<p>The “age” feature is straightforward, it’s simply the patient’s current age at the time of measuring their vitals. As
    one might expect, the older the patient, the higher the probability of passing away during the follow-up period. The
    distribution of age itself roughly follows normal distribution with a sharp decline past 70 years old.</p>
<img class="graph" alt="Histogram of age versus DEATH_EVENT" src="https://i.imgur.com/je3ZSAs.png">
<p>The “serum_creatinine” feature represents the amount of serum creatinine in the blood in mg/dL. Serum creatinine
    levels can be an important marker of kidney function in people with heart failure. Heart failure can cause decreased
    blood flow to the kidneys, which can lead to impaired kidney function and higher serum creatinine levels.
    Distribution is concentrated between 0 and 2, which is likely within the average range of serum creatinine levels in
    a normal human body. Proportionately, patients exceeding roughly 1.5mg/dL of serum creatinine are much likelier to
    pass away during the follow-up period. </p>
<img class="graph" alt="Histogram of serum_creatinine versus DEATH_EVENT" src="https://i.imgur.com/jm065pB.png">
<p>The “serum_sodium” feature indicates the level of serum sodium in the blood in mEq/L. In heart failure, there can be
    a variety of changes in sodium and fluid balance in the body, which can affect the heart's ability to pump
    effectively. Hyponatremia, which is a low level of sodium in the blood, can be a common finding in people with heart
    failure. Less commonly, hypernatremia, which is a high level of sodium in the blood, can also occur in people with
    heart failure, especially in those with impaired kidney function. Indeed, the distribution of serum sodium versus
    DEATH_EVENT supports these medical observations, with proportionately the least number of deaths within about 2
    standard deviations of the normally distributed histogram.</p>
<img class="graph" alt="Histogram of serum_sodium versus DEATH_EVENT" src="https://i.imgur.com/pZecwpZ.png">
<p>The “ejection_fraction” feature represents the percentage of blood leaving the heart at each contraction. In a
    healthy heart, the ejection fraction is typically between 50% and 70%, while in heart failure, the ejection fraction
    may be reduced. Indeed, the histogram supports these findings as there are proportionately a lot more deaths as
    ejection fraction drops below 40%.</p>
<img class="graph" alt="Histogram of serum_creatinine versus DEATH_EVENT" src="https://i.imgur.com/mU04cbN.png">
<p>As such, every one of the significant variables have some kind of medical basis for their significance which is
    highly encouraging, we can have good confidence that our models are using good features.</p>
<h3 class="text-secondary">Algorithm Selection</h3>
<p>Using the 5 significant features, a total of 6 algorithms were used to develop models, as follows:</p>
<ol>
    <li>LogisticRegression</li>
    <li>MLPClassifier</li>
    <li>KNeighborsClassifier</li>
    <li>RandomForestClassifier</li>
    <li>GaussianNB</li>
    <li>StackingClassifier</li>
</ol>
<p>The only model that benefited from scaling was the MLP classifier model, so a StandardScaler was used for its model
    training. The MLPClassifier model best parameters were grid searched with 1000 max iterations and early stopping.
    The StackingClassifier algorithm combines all the other algorithms, using LogisticRegression as the meta
    classifier.</p>
<p>All models are built with cross-fold validation, and the scores are averages between 100 runs.</p>
<p>Model performances for each algorithm (corresponding to their number in the above list) are shown below:</p>
<table class="table table-striped table-bordered">
    <thead class="thead-dark">
    <tr>
        <th></th>
        <th>1</th>
        <th>2</th>
        <th>3</th>
        <th>4</th>
        <th>5</th>
        <th>6</th>
    </tr>
    </thead>
    <tbody>
    <tr>
        <td>Average accuracy:</td>
        <td>0.8274</td>
        <td>0.8274</td>
        <td>0.8396</td>
        <td>0.8336</td>
        <td>0.7973</td>
        <td>0.8359</td>
    </tr>
    <tr>
        <td>Accuracy std:</td>
        <td>0.0328</td>
        <td>0.0313</td>
        <td>0.0289</td>
        <td>0.0338</td>
        <td>0.0425</td>
        <td>0.0298</td>
    </tr>
    <tr>
        <td>Average precision:</td>
        <td>0.7602</td>
        <td>0.7602</td>
        <td>0.8081</td>
        <td>0.7528</td>
        <td>0.7604</td>
        <td>0.7571</td>
    </tr>
    <tr>
        <td>Precision std:</td>
        <td>0.0711</td>
        <td>0.0758</td>
        <td>0.0713</td>
        <td>0.0762</td>
        <td>0.0759</td>
        <td>0.0761</td>
    </tr>
    <tr>
        <td>Average recall:</td>
        <td>0.6806</td>
        <td>0.6821</td>
        <td>0.6600</td>
        <td>0.7249</td>
        <td>0.5412</td>
        <td>0.7276</td>
    </tr>
    <tr>
        <td>Recall std:</td>
        <td>0.0777</td>
        <td>0.0749</td>
        <td>0.0730</td>
        <td>0.0785</td>
        <td>0.0995</td>
        <td>0.0735</td>
    </tr>
    <tr>
        <td>Average f1:</td>
        <td>0.7147</td>
        <td>0.7151</td>
        <td>0.7230</td>
        <td>0.7345</td>
        <td>0.6275</td>
        <td>0.7378</td>
    </tr>
    <tr>
        <td>f1 std:</td>
        <td>0.0563</td>
        <td>0.0537</td>
        <td>0.0532</td>
        <td>0.0555</td>
        <td>0.0810</td>
        <td>0.0494</td>
    </tr>
    </tbody>
</table>
<p>The model performances for each algorithm are competitive, but we opted to use KNeighborsClassifier as our algorithm
    of choice because it was very close behind StackingClassifier but much simpler to use.</p>
<h3 class="text-secondary">Model Evaluation</h3>
<p>Here are the performance metrics for the KNeighborsClassifier model again:</p>
<table class="table table-striped table-bordered">
    <thead class="thead-dark">
    <tr>
        <th></th>
        <th>KNeighborsClassifier</th>
    </tr>
    </thead>
    <tbody>
    <tr>
        <td>Average accuracy:</td>
        <td>0.8396</td>
    </tr>
    <tr>
        <td>Accuracy std:</td>
        <td>0.0289</td>
    </tr>
    <tr>
        <td>Average precision:</td>
        <td>0.8081</td>
    </tr>
    <tr>
        <td>Precision std:</td>
        <td>0.0713</td>
    </tr>
    <tr>
        <td>Average recall:</td>
        <td>0.6600</td>
    </tr>
    <tr>
        <td>Recall std:</td>
        <td>0.0730</td>
    </tr>
    <tr>
        <td>Average f1:</td>
        <td>0.7230</td>
    </tr>
    <tr>
        <td>f1 std:</td>
        <td>0.0532</td>
    </tr>
    </tbody>
</table>
{% endblock content %}